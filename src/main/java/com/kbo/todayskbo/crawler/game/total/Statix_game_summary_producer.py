<<<<<<< HEAD
#game-result-meta
#- gameId, gameDate, stadium, awayTeamName, homeTeamName, awayScore, homeScore, winner, loser, save

#game-inning-scores
#- gameId, teamName, inning, runs, awayTeamName, homeTeamName

#game-total-stats
#- gameId, teamName, type (R/H/E/B), value, awayTeamName, homeTeamName


=======
>>>>>>> feature/crawler-schedule
import logging
import time
import random
import re
import json
import csv
import os
from datetime import datetime
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from kafka import KafkaProducer
from multiprocessing import Pool

# Î°úÍ∑∏ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)

# Kafka ÏÑ§Ï†ï
producer = KafkaProducer(
    bootstrap_servers=['localhost:9094'],
    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
)

def send_to_kafka(topic, data):
    logging.info(f"üì§ Kafka Ï†ÑÏÜ° ÏôÑÎ£å [{topic}]: {data}")
    producer.send(topic, value=data)

def is_valid_team_name(name):
    return bool(re.fullmatch(r'[\uac00-\ud7a3A-Z]{2,10}', name))

def extract_team_data(row):
    try:
        cols = row.find_elements(By.TAG_NAME, "td")
        name = cols[0].text.strip()
        if not is_valid_team_name(name):
            return None, [], {}
        inning_scores = []
        for td in cols[1:-4]:
            try:
                score_div = td.find_element(By.CLASS_NAME, "score")
                score_text = score_div.text.strip().split('\n')[0]
                inning_scores.append(score_text if score_text.isdigit() else '0')
            except:
                inning_scores.append('0')
        totals = {
            "R": cols[-4].text.strip(),
            "H": cols[-3].text.strip(),
            "E": cols[-2].text.strip(),
            "B": cols[-1].text.strip()
        }
        return name, inning_scores, totals
    except Exception as e:
        logging.warning(f"‚ùå ÌåÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        return None, [], {}

def extract_game_meta(driver):
    try:
        box_head = driver.find_element(By.CSS_SELECTOR, ".box_head").text.strip()
        game_date, stadium = box_head.split()
        return game_date.strip(), stadium.strip("()")
    except:
        return None, None

def extract_result_players(driver):
    try:
        win = driver.find_element(By.CSS_SELECTOR, ".game_result .win a").text.strip()
        lose = driver.find_element(By.CSS_SELECTOR, ".game_result .lose a").text.strip()
        save_elem = driver.find_elements(By.CSS_SELECTOR, ".game_result .save a")
        save = save_elem[0].text.strip() if save_elem else None
        return win, lose, save
    except:
        return None, None, None

<<<<<<< HEAD
def log_success(game_id, year):
    with open(f"success_log_{year}.csv", "a", newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([game_id, datetime.now().isoformat()])

def save_checkpoint(year, i):
    with open(f"checkpoint_{year}.txt", "w") as f:
        f.write(str(i))

def load_checkpoint(year):
    path = f"checkpoint_{year}.txt"
    return int(open(path).read()) if os.path.exists(path) else 1

def crawl_game_inning_score(s_no, year):
    url = f"https://statiz.sporki.com/schedule/?m=summary&s_no={s_no}"
    logging.info(f"üìÖ ÌÅ¨Î°§ÎßÅ ÏãúÏûë: s_no={s_no}")
=======
def determine_status(rows):
    text = " ".join(row.text for row in rows)
    if "ÏΩúÎìú" in text:
        return "ÏΩúÎìúÍ≤åÏûÑ"
    elif "ÏßÄÏó∞" in text or "ÎîúÎ†àÏù¥" in text:
        return "ÎîúÎ†àÏù¥"
    elif "Ï¢ÖÎ£å" in text or "Ïäπ" in text:
        return "Í≤ΩÍ∏∞Ï¢ÖÎ£å"
    else:
        return "ÏßÑÌñâÏ§ë"

def crawl_game_inning_score(s_no, year):
    url = f"https://statiz.sporki.com/schedule/?m=summary&s_no={s_no}"
    logging.info(f"üóïÔ∏è ÌÅ¨Î°§ÎßÅ ÏãúÏûë: s_no={s_no}")
>>>>>>> feature/crawler-schedule

    options = uc.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
<<<<<<< HEAD

=======
>>>>>>> feature/crawler-schedule
    driver = uc.Chrome(options=options)

    try:
        driver.get(url)
        time.sleep(1.5)
        rows = driver.find_elements(By.CSS_SELECTOR, ".table_type03 tbody tr")
        game_date, stadium = extract_game_meta(driver)

<<<<<<< HEAD
        if not rows and game_date:
=======
        weekday_str = None
        if game_date:
>>>>>>> feature/crawler-schedule
            game_dt = datetime.strptime(game_date, "%Y-%m-%d").date()
            weekday_str = ["Ïõî", "Ìôî", "Ïàò", "Î™©", "Í∏à", "ÌÜ†", "Ïùº"][game_dt.weekday()]

        if not rows and game_date:
            if game_dt < datetime.today().date():
                send_to_kafka("game-result-meta", {
                    "gameId": s_no,
                    "status": "Í≤ΩÍ∏∞Ï∑®ÏÜå",
                    "gameDate": game_date,
                    "weekday": weekday_str,
                    "stadium": stadium
                })
                return False
<<<<<<< HEAD
            return True
=======
            else:
                send_to_kafka("game-result-meta", {
                    "gameId": s_no,
                    "status": "Í≤ΩÍ∏∞ÏòàÏ†ï",
                    "gameDate": game_date,
                    "weekday": weekday_str,
                    "stadium": stadium
                })
                return True
>>>>>>> feature/crawler-schedule

        if len(rows) < 2:
            logging.warning(f"‚ùå ÌåÄ Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±: {s_no}")
            return True

        valid_teams, team_scores, team_totals = [], {}, {}
<<<<<<< HEAD

=======
>>>>>>> feature/crawler-schedule
        for row in rows:
            name, scores, totals = extract_team_data(row)
            if name:
                valid_teams.append(name)
                team_scores[name] = scores
                team_totals[name] = totals

        if len(valid_teams) != 2 or any(t == "Ï†ïÍ∑ú" for t in valid_teams):
            logging.warning(f"‚ùå Ïú†Ìö® ÌåÄ Î∂ÄÏ°± ÎòêÎäî ÎπÑÏ†ïÏÉÅ ÌåÄÎ™Ö: s_no={s_no}, teams={valid_teams}")
            if game_date:
                send_to_kafka("game-result-meta", {
                    "gameId": s_no,
                    "status": "Í≤ΩÍ∏∞Ï∑®ÏÜå",
                    "gameDate": game_date,
                    "weekday": weekday_str,
                    "stadium": stadium
                })
            return True

        away, home = valid_teams
        winner, loser, save = extract_result_players(driver)
        game_id = s_no
        away_score = int(team_totals[away]["R"]) if team_totals[away]["R"].isdigit() else 0
        home_score = int(team_totals[home]["R"]) if team_totals[home]["R"].isdigit() else 0

<<<<<<< HEAD
        for team in [away_team, home_team]:
=======
        for team in [away, home]:
>>>>>>> feature/crawler-schedule
            for inning, run in enumerate(team_scores[team], start=1):
                send_to_kafka("game-inning-scores", {
                    "gameId": game_id,
                    "teamName": team,
                    "inning": inning,
                    "runs": int(run),
<<<<<<< HEAD
                    "awayTeamName": away_team,
                    "homeTeamName": home_team
                })

        for team in [away_team, home_team]:
=======
                    "awayTeamName": away,
                    "homeTeamName": home
                })

        for team in [away, home]:
>>>>>>> feature/crawler-schedule
            for key in ['R', 'H', 'E', 'B']:
                send_to_kafka("game-total-stats", {
                    "gameId": game_id,
                    "teamName": team,
                    "awayTeamName": away,
                    "homeTeamName": home,
                    "type": key,
                    "value": int(team_totals[team][key]) if team_totals[team][key].isdigit() else 0
                })

<<<<<<< HEAD
=======
        game_status = determine_status(rows)
>>>>>>> feature/crawler-schedule
        send_to_kafka("game-result-meta", {
            "gameId": game_id,
            "gameDate": game_date,
            "weekday": weekday_str,
            "stadium": stadium,
            "awayTeamName": away,
            "homeTeamName": home,
            "awayScore": away_score,
            "homeScore": home_score,
            "winner": winner,
            "loser": loser,
<<<<<<< HEAD
            "save": save
        })
=======
            "save": save,
            "status": game_status
        })

        with open(f"success_log_{year}.csv", "a", newline='') as csvfile:
            csv.writer(csvfile).writerow([game_id, datetime.now().isoformat()])
>>>>>>> feature/crawler-schedule

        log_success(game_id, year)
        return True
    except Exception as e:
        logging.error(f"‚ùå ÏòàÏô∏ Î∞úÏÉù: s_no={s_no}, error={e}")
        return True
    finally:
        driver.quit()

<<<<<<< HEAD
# Ïó∞ÎèÑÎ≥Ñ ÏàúÏ∞® ÌÅ¨Î°§ÎßÅ Ìï®Ïàò
=======
def load_checkpoint(year):
    path = f"checkpoint_{year}.txt"
    return int(open(path).read()) if os.path.exists(path) else 1

def save_checkpoint(year, i):
    with open(f"checkpoint_{year}.txt", "w") as f:
        f.write(str(i))

>>>>>>> feature/crawler-schedule
def batch_crawl_until_end(year):
    i = load_checkpoint(year)
    while True:
        s_no = int(f"{year}{str(i).zfill(4)}")
        keep_going = crawl_game_inning_score(s_no, year)
<<<<<<< HEAD

        if keep_going is False:
            logging.info(f"‚úÖ ÌÅ¨Î°§ÎßÅ Ï¢ÖÎ£å Ï°∞Í±¥ ÎèÑÎã¨: s_no={s_no}")
            break

        save_checkpoint(year, i)
        sleep_time = random.uniform(1.5, 3.5)
        logging.info(f"‚è± {sleep_time:.2f}Ï¥à ÎåÄÍ∏∞ ÌõÑ Îã§Ïùå Í≤ΩÍ∏∞Î°ú...")
        time.sleep(sleep_time)
        i += 1

# Î©ÄÌã∞ ÌîÑÎ°úÏÑ∏Ïä§Ïö© Ìï®Ïàò (Îã®Ïùº Ïó∞ÎèÑ ÏûëÏóÖÏûê)
def crawl_for_year(year):
    batch_crawl_until_end(year)

# ‚úÖ ÏßÑÏßú Ïã§ÌñâÎ∂Ä ‚Äî Î≥ëÎ†¨Îßå Ïã§Ìñâ!
if __name__ == "__main__":
    years = list(range(2024, 2025))
    with Pool(processes=3) as pool:
        pool.map(crawl_for_year, years)
=======
        if not keep_going:
            logging.info(f"‚úÖ ÌÅ¨Î°§ÎßÅ Ï¢ÖÎ£å: s_no={s_no}")
            break
        save_checkpoint(year, i)
        time.sleep(random.uniform(2, 5))
        i += 1

def crawl_for_year(year):
    batch_crawl_until_end(year)

#if __name__ == "__main__":
#    years = list(range(2024, 2025))
#    with Pool(processes=3) as pool:
#        pool.map(crawl_for_year, years)
if __name__ == "__main__":
    years = [2025]  # ‚úÖ Ïó¨Í∏∞Îßå Ïù¥Î†áÍ≤å ÏàòÏ†ïÌïòÎ©¥ Îê®
    with Pool(processes=3) as pool:
        pool.map(crawl_for_year, years)
>>>>>>> feature/crawler-schedule
